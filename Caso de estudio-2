{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1784421,"sourceType":"datasetVersion","datasetId":1060815}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile config.py\n#Archivo de configuracion de parametros\n# --- Par√°metros del Dataset ---\nIMAGENET_TRAIN_PATH = '/kaggle/working/imagewoof2/train'\nNUM_CLASSES = 10 \n\n# --- Par√°metros de Entrenamiento ---\nLEARNING_RATE = 0.01\nMOMENTUM = 0.9\nBATCH_SIZE = 32\nNUM_EPOCHS = 10\n\n# --- Par√°metros de DC-ASGD-a ---\nADAPTIVE_ALPHA = 0.1 \n\n# --- Par√°metros de Arquitectura Distribuida ---\nMASTER_ADDR = 'localhost'\nMASTER_PORT = '29501' \nWORLD_SIZE = 2\nPARAMETER_SERVER_RANK = 0","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:36:56.719469Z","iopub.execute_input":"2025-10-24T16:36:56.719883Z","iopub.status.idle":"2025-10-24T16:36:56.725502Z","shell.execute_reply.started":"2025-10-24T16:36:56.719834Z","shell.execute_reply":"2025-10-24T16:36:56.724803Z"}},"outputs":[{"name":"stdout","text":"Overwriting config.py\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"%%writefile dataset.py\n\nimport torch\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, distributed\n\ndef get_imagenet_loader(rank, world_size, batch_size, train_path):\n    \n    transform = transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                           std=[0.229, 0.224, 0.225]),\n    ])\n\n    train_dataset = datasets.ImageFolder(\n        root=train_path,\n        transform=transform\n    )\n\n    train_sampler = distributed.DistributedSampler(\n        train_dataset,\n        num_replicas=world_size - 1,\n        rank=rank - 1\n    )\n\n    # ‚úÖ CORRECCI√ìN: num_workers=0 para evitar deadlocks en entorno distribuido\n    train_loader = DataLoader(\n        dataset=train_dataset,\n        batch_size=batch_size,\n        sampler=train_sampler,\n        num_workers=0,  # ‚Üê CAMBIAR DE 2 A 0\n        pin_memory=True\n    )\n\n    return train_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:36:56.726629Z","iopub.execute_input":"2025-10-24T16:36:56.726793Z","iopub.status.idle":"2025-10-24T16:36:56.749374Z","shell.execute_reply.started":"2025-10-24T16:36:56.726779Z","shell.execute_reply":"2025-10-24T16:36:56.748702Z"}},"outputs":[{"name":"stdout","text":"Overwriting dataset.py\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"%%writefile model.py\n#Aqui se contiene el modelo a usar el cual es el modelo Alexnet\n\nimport torchvision.models as models\n\ndef build_model(num_classes=10):\n    model = models.alexnet(pretrained=False, num_classes=num_classes)\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:36:56.750160Z","iopub.execute_input":"2025-10-24T16:36:56.750443Z","iopub.status.idle":"2025-10-24T16:36:56.762114Z","shell.execute_reply.started":"2025-10-24T16:36:56.750420Z","shell.execute_reply":"2025-10-24T16:36:56.761290Z"}},"outputs":[{"name":"stdout","text":"Overwriting model.py\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"%%writefile optimizer.py\n#Optimizador donde se desarrolla la logica del (DC-ASGD-a)\nimport torch\n\ndef get_model_weights_copy(model):\n    \"\"\"Devuelve una copia en CPU de los pesos del modelo.\"\"\"\n    with torch.no_grad():\n        return [p.data.clone().cpu() for p in model.parameters()]\n\ndef get_model_gradients(model):\n    \"\"\"Devuelve una copia en CPU de los gradientes del modelo.\"\"\"\n    with torch.no_grad():\n        return [p.grad.data.clone().cpu() for p in model.parameters() if p.grad is not None]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:36:56.763412Z","iopub.execute_input":"2025-10-24T16:36:56.763972Z","iopub.status.idle":"2025-10-24T16:36:56.774320Z","shell.execute_reply.started":"2025-10-24T16:36:56.763954Z","shell.execute_reply":"2025-10-24T16:36:56.773733Z"}},"outputs":[{"name":"stdout","text":"Overwriting optimizer.py\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"#Logger\nimport time\nimport json\nfrom pathlib import Path\n\nclass TrainingLogger:\n    def __init__(self, log_dir=\"logs\", rank=0):\n        self.rank = rank\n        self.log_dir = Path(log_dir)\n        self.log_dir.mkdir(exist_ok=True)\n        \n        self.log_file = self.log_dir / f\"worker_{rank}.jsonl\"\n        self.start_time = time.time()\n        self.metrics = {'losses': [], 'batch_times': []}\n    \n    def log_batch(self, epoch, batch_idx, loss, batch_time=None):\n        entry = {\n            'timestamp': time.time() - self.start_time,\n            'epoch': epoch,\n            'batch': batch_idx,\n            'loss': loss,\n            'batch_time': batch_time\n        }\n        \n        with open(self.log_file, 'a') as f:\n            f.write(json.dumps(entry) + '\\n')\n        \n        self.metrics['losses'].append(loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:36:56.775058Z","iopub.execute_input":"2025-10-24T16:36:56.775265Z","iopub.status.idle":"2025-10-24T16:36:56.787339Z","shell.execute_reply.started":"2025-10-24T16:36:56.775251Z","shell.execute_reply":"2025-10-24T16:36:56.786614Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"%%writefile validation.py\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport config\n\ndef validate_model(model, device, val_path=None):\n    \"\"\"\n    Eval√∫a el modelo en el conjunto de validaci√≥n.\n    \"\"\"\n    model.eval()\n    \n    if val_path is None:\n        val_path = config.IMAGENET_TRAIN_PATH.replace('train', 'val')\n    \n    transform = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                           std=[0.229, 0.224, 0.225]),\n    ])\n    \n    try:\n        val_dataset = datasets.ImageFolder(root=val_path, transform=transform)\n        # --- 1. CAMBIAMOS num_workers DE 2 A 0 ---\n        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n    except FileNotFoundError:\n        print(f\"‚ö†Ô∏è  No se encontr√≥ conjunto de validaci√≥n en: {val_path}. Saltando validaci√≥n.\")\n        return None\n    except Exception as e:\n        print(f\"‚ö†Ô∏è  Error cargando validaci√≥n: {e}. Saltando validaci√≥n.\")\n        return None\n    \n    correct = 0\n    total = 0\n    val_loss = 0.0\n    \n    with torch.no_grad():\n        for data, target in val_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            \n            val_loss += F.cross_entropy(output, target, reduction='sum').item()\n            _, pred = torch.max(output, 1)\n            total += target.size(0)\n            correct += (pred == target).sum().item()\n            \n    if total == 0:\n        print(\"‚ö†Ô∏è  El dataset de validaci√≥n est√° vac√≠o.\")\n        return {'avg_loss': 0, 'accuracy': 0}\n\n    avg_loss = val_loss / total\n    accuracy = 100. * correct / total\n    \n    return {\n        'avg_loss': avg_loss,\n        'accuracy': accuracy\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:36:56.788098Z","iopub.execute_input":"2025-10-24T16:36:56.788450Z","iopub.status.idle":"2025-10-24T16:36:56.797995Z","shell.execute_reply.started":"2025-10-24T16:36:56.788419Z","shell.execute_reply":"2025-10-24T16:36:56.797282Z"}},"outputs":[{"name":"stdout","text":"Overwriting validation.py\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"%%writefile parameter_server.py\n\nimport os\nimport torch\nimport torch.distributed as dist\nimport torch.optim as optim\nimport argparse\n\ndef run_parameter_server(rank, world_size, config_dict):\n    print(f\"[DEBUG-PS] Rank {rank}: Iniciando...\")\n    \n    os.environ['MASTER_ADDR'] = config_dict['MASTER_ADDR']\n    os.environ['MASTER_PORT'] = config_dict['MASTER_PORT']\n    \n    try:\n        dist.init_process_group(backend='gloo', rank=rank, world_size=world_size)\n        print(f\"[DEBUG-PS] Rank {rank}: Process group inicializado\")\n    except Exception as e:\n        print(f\"[ERROR-PS] Error inicializando grupo de procesos: {e}\")\n        return\n\n    device = torch.device(\"cpu\")\n    #print(f\"\\n[DEBUG-PS] Rank {rank}: Usando {device}\\n\")\n    \n    from model import build_model\n    \n    #print(f\"[DEBUG-PS] Construyendo modelo...\")\n    model = build_model(config_dict['NUM_CLASSES']).to(device)\n    #print(f\"[DEBUG-PS] Modelo construido\")\n    \n    optimizer = optim.SGD(\n        model.parameters(), \n        lr=config_dict['LEARNING_RATE'], \n        momentum=config_dict['MOMENTUM']\n    )\n\n    #print(f\"[DEBUG-PS] Listo y esperando workers\")\n\n    total_steps = 0\n    num_workers = world_size - 1\n    finished_workers = 0\n    \n    while finished_workers < num_workers: \n        try:\n            #print(f\"[DEBUG-PS] Esperando solicitud...\")\n            request_tensor = torch.empty(1, dtype=torch.int)\n            sender_rank = dist.recv(request_tensor, src=None)\n            #print(f\"[DEBUG-PS] Solicitud recibida de Worker {sender_rank}\")\n            \n            if request_tensor.item() == -1:\n               # print(f\"[DEBUG-PS] Worker {sender_rank} ha terminado\")\n                finished_workers += 1\n                continue\n            \n            # Enviar pesos actuales\n            #print(f\"[DEBUG-PS] Enviando pesos a Worker {sender_rank}...\")\n            current_weights = [p.data.clone().cpu() for p in model.parameters()]\n            dist.send_object_list([current_weights], dst=sender_rank)\n            #print(f\"[DEBUG-PS] Pesos enviados\")\n\n            # RECIBIR COMO DICCIONARIO\n            #print(f\"[DEBUG-PS] Esperando gradientes de Worker {sender_rank}...\")\n            received_data = [None]\n            dist.recv_object_list(received_data, src=sender_rank)\n            #print(f\"[DEBUG-PS] Datos recibidos, extrayendo...\")\n            \n            # EXTRAER DEL DICCIONARIO\n            data = received_data[0]\n            gradients = data['gradients']\n            weight_diffs = data['weight_diffs']\n            #print(f\"[DEBUG-PS] Gradientes y diffs extra√≠dos correctamente\")\n            \n            # Calcular norma de diferencia de pesos\n            optimizer.zero_grad()\n            weight_diff_norm_sq = 0.0\n            \n            with torch.no_grad():\n                for diff in weight_diffs:\n                    weight_diff_norm_sq += torch.sum(diff ** 2)\n            \n            weight_diff_norm = torch.sqrt(weight_diff_norm_sq)\n            lambda_t = config_dict['ADAPTIVE_ALPHA'] * weight_diff_norm\n            \n            # Aplicar actualizaci√≥n con compensaci√≥n de retraso\n            for i, param in enumerate(model.parameters()):\n                compensated_grad = gradients[i].to(device) + lambda_t * weight_diffs[i].to(device)\n                param.grad = compensated_grad\n            \n            optimizer.step()\n            \n            total_steps += 1\n            if total_steps % 10 == 0: \n                print(f\"[PS] Step {total_steps} | Œª_t = {lambda_t.item():.4f}\")\n        \n        except Exception as e:\n            print(f\"[ERROR-PS] Error: {e}\")\n            import traceback\n            traceback.print_exc()\n            break\n    \n    print(f\"[DEBUG-PS] Todos los workers terminaron. Cerrando...\")\n    dist.destroy_process_group()\n\nif __name__ == \"__main__\":\n    print(\"[DEBUG-PS] Script parameter_server.py iniciado\")\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--rank\", type=int, required=True)\n    parser.add_argument(\"--world_size\", type=int, required=True)\n    args = parser.parse_args()\n    \n    print(f\"[DEBUG-PS] Args: rank={args.rank}, world_size={args.world_size}\")\n    \n    import config\n    config_dict = {\n        'MASTER_ADDR': config.MASTER_ADDR,\n        'MASTER_PORT': config.MASTER_PORT,\n        'NUM_CLASSES': config.NUM_CLASSES,\n        'LEARNING_RATE': config.LEARNING_RATE,\n        'MOMENTUM': config.MOMENTUM,\n        'ADAPTIVE_ALPHA': config.ADAPTIVE_ALPHA\n    }\n    \n    print(f\"[DEBUG-PS] Config cargado, llamando run_parameter_server()...\")\n    run_parameter_server(args.rank, args.world_size, config_dict)\n    print(f\"[DEBUG-PS] run_parameter_server() completado\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:36:56.799085Z","iopub.execute_input":"2025-10-24T16:36:56.799352Z","iopub.status.idle":"2025-10-24T16:36:56.811319Z","shell.execute_reply.started":"2025-10-24T16:36:56.799332Z","shell.execute_reply":"2025-10-24T16:36:56.810751Z"}},"outputs":[{"name":"stdout","text":"Overwriting parameter_server.py\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"%%writefile worker.py\n\nimport os\nimport torch\nimport torch.distributed as dist\nimport torch.nn.functional as F\nimport argparse\nimport time  # <--- 1. Importamos 'time'\n\n# --- Importaciones de nuestros m√≥dulos ---\nimport config\nfrom model import build_model\nfrom dataset import get_imagenet_loader\nfrom optimizer import get_model_weights_copy, get_model_gradients\nfrom validation import validate_model  # <--- 2. Importamos 'validate_model'\n\ndef run_worker(rank, world_size, config_dict):\n    # --- 3. Iniciamos el contador de tiempo total ---\n    total_start_time = time.time()\n    \n    os.environ['MASTER_ADDR'] = config_dict['MASTER_ADDR']\n    os.environ['MASTER_PORT'] = config_dict['MASTER_PORT']\n    \n    try:\n        dist.init_process_group(backend='gloo', rank=rank, world_size=world_size)\n    except Exception as e:\n        print(f\"[ERROR] Worker {rank}: Error inicializando grupo: {e}\")\n        return\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"\\n--- Worker {rank}: Usando dispositivo: {device} ---\")\n    \n    model = build_model(config_dict['NUM_CLASSES']).to(device)\n    \n    try:\n        train_loader = get_imagenet_loader(\n            rank, \n            world_size, \n            config_dict['BATCH_SIZE'],\n            config_dict['TRAIN_PATH']\n        )\n    except Exception as e:\n        print(f\"[ERROR] Worker {rank}: Error cargando datos: {e}\")\n        return\n\n    print(f\"--- Worker {rank}: Iniciando entrenamiento de {config_dict['NUM_EPOCHS']} √©pocas ---\")\n\n    for epoch in range(config_dict['NUM_EPOCHS']):\n        \n        # --- 4. M√©tricas por √©poca ---\n        epoch_start_time = time.time()\n        epoch_total_loss = 0.0\n        batch_count = 0\n        \n        train_loader.sampler.set_epoch(epoch)\n        \n        for i, (data, target) in enumerate(train_loader):\n            try:\n                # 1. Guardar pesos locales ANTES de solicitar los nuevos\n                weights_t_minus_tau = get_model_weights_copy(model)\n\n                # 2. Solicitar pesos actualizados (Protocolo Paso 1)\n                dist.send(torch.tensor([rank], dtype=torch.int), dst=0)\n                \n                # 3. Recibir pesos del PS (Protocolo Paso 2)\n                received_weights = [None]\n                dist.recv_object_list(received_weights, src=0)\n                weights_t = received_weights[0]\n                \n                # 4. Actualizar modelo local con los pesos recibidos\n                with torch.no_grad():\n                    for j, param in enumerate(model.parameters()):\n                        param.data.copy_(weights_t[j].to(device))\n                \n                # 5. Computo (Forward + Backward)\n                data, target = data.to(device), target.to(device)\n                model.zero_grad()\n                output = model(data)\n                loss = F.cross_entropy(output, target)\n                loss.backward()\n                \n                # 6. Acumular m√©tricas de la √©poca\n                epoch_total_loss += loss.item()\n                batch_count += 1\n                \n                # 7. Extraer gradientes\n                gradients = get_model_gradients(model)\n                \n                # 8. Calcular diferencias (Pesos Globales - Pesos Locales Anteriores)\n                weight_diffs = [wt - w_tm_tau for wt, w_tm_tau in zip(weights_t, weights_t_minus_tau)]\n\n                # 9. Enviar gradientes y diffs al PS (Protocolo Paso 3)\n                data_to_send = {\n                    'gradients': gradients,\n                    'weight_diffs': weight_diffs\n                }\n                dist.send_object_list([data_to_send], dst=0)\n                \n                # --- 5. Eliminamos el print de cada batch ---\n                # (El print que estaba aqu√≠ fue removido)\n            \n            except Exception as e:\n                # Simplificamos el error\n                print(f\"[ERROR] Worker {rank} en Epoch {epoch+1} Batch {i}: {e}\")\n                continue\n        \n        # --- 6. Imprimimos el resumen de la √©poca ---\n        epoch_end_time = time.time()\n        epoch_duration = epoch_end_time - epoch_start_time\n        avg_loss = epoch_total_loss / batch_count if batch_count > 0 else 0\n        \n        print(f\"‚úÖ Worker {rank} | Epoch {epoch+1}/{config_dict['NUM_EPOCHS']} | Loss Prom: {avg_loss:.4f} | Duraci√≥n: {epoch_duration:.2f}s\")\n\n    # --- 7. Resumen final (Tiempo y Accuracy) ---\n    print(f\"--- Worker {rank}: Entrenamiento finalizado ---\")\n    print(f\"--- Worker {rank}: Calculando accuracy final... ---\")\n    \n    # Ponemos el modelo en modo evaluaci√≥n\n    model.eval()\n    val_results = validate_model(model, device, config_dict['VAL_PATH'])\n    \n    if val_results:\n        print(f\"üéØ Accuracy Final: {val_results['accuracy']:.2f}%\")\n        print(f\"üìâ Loss Final (Val): {val_results['avg_loss']:.4f}\")\n    else:\n        print(\"‚ö†Ô∏è No se pudo calcular la validaci√≥n (¬øDataset 'val' no encontrado?)\")\n\n    total_end_time = time.time()\n    total_duration = total_end_time - total_start_time\n    print(f\"‚è±Ô∏è Tiempo Total de Entrenamiento: {total_duration:.2f}s\")\n    \n    # --- Finalizaci√≥n ---\n    dist.send(torch.tensor([-1], dtype=torch.int), dst=0)\n    dist.destroy_process_group()\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--rank\", type=int, required=True)\n    parser.add_argument(\"--world_size\", type=int, required=True)\n    args = parser.parse_args()\n    \n    # Cargar configuraci√≥n desde config.py\n    config_dict = {\n        'MASTER_ADDR': config.MASTER_ADDR,\n        'MASTER_PORT': config.MASTER_PORT,\n        'NUM_CLASSES': config.NUM_CLASSES,\n        'LEARNING_RATE': config.LEARNING_RATE,\n        'MOMENTUM': config.MOMENTUM,\n        'BATCH_SIZE': config.BATCH_SIZE,\n        'NUM_EPOCHS': config.NUM_EPOCHS,\n        'TRAIN_PATH': config.IMAGENET_TRAIN_PATH,\n        # --- 8. A√±adimos la ruta de validaci√≥n al config ---\n        'VAL_PATH': config.IMAGENET_TRAIN_PATH.replace('train', 'val')\n    }\n    \n    run_worker(args.rank, args.world_size, config_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:36:56.812456Z","iopub.execute_input":"2025-10-24T16:36:56.812713Z","iopub.status.idle":"2025-10-24T16:36:56.829563Z","shell.execute_reply.started":"2025-10-24T16:36:56.812692Z","shell.execute_reply":"2025-10-24T16:36:56.828910Z"}},"outputs":[{"name":"stdout","text":"Overwriting worker.py\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import subprocess\nimport time\n\nprint(\"üßπ Limpiando procesos anteriores...\")\n\nsubprocess.run(['pkill', '-9', '-f', 'parameter_server.py'], \n               stderr=subprocess.DEVNULL, stdout=subprocess.DEVNULL)\nsubprocess.run(['pkill', '-9', '-f', 'worker.py'], \n               stderr=subprocess.DEVNULL, stdout=subprocess.DEVNULL)\n\ntime.sleep(3)\nprint(\"‚úÖ Listo para entrenar\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:36:56.831199Z","iopub.execute_input":"2025-10-24T16:36:56.831437Z","iopub.status.idle":"2025-10-24T16:36:59.850811Z","shell.execute_reply.started":"2025-10-24T16:36:56.831418Z","shell.execute_reply":"2025-10-24T16:36:59.850166Z"}},"outputs":[{"name":"stdout","text":"üßπ Limpiando procesos anteriores...\n‚úÖ Listo para entrenar\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# launcher.py\nimport subprocess\nimport os\nimport time\nfrom pathlib import Path\nimport sys  # <--- 1. IMPORTAMOS SYS\n\nclass DistributedTrainingLauncher:\n    def __init__(self, world_size=2):\n        self.world_size = world_size\n        self.processes = []\n        \n    def prepare_data(self):\n        target_dir = Path(\"/kaggle/working/imagewoof2\")\n        train_dir = target_dir / \"train\"\n        tar_file = Path(\"/kaggle/input/imagewoof/imagewoof2.tgz\")\n        \n        if train_dir.exists():\n            print(f\"‚úì Dataset ya existe\")\n            return True\n        \n        print(f\"Descomprimiendo dataset...\")\n        target_dir.mkdir(parents=True, exist_ok=True)\n        \n        try:\n            subprocess.run(['tar', '-xzf', str(tar_file), '-C', '/kaggle/working/'], check=True)\n            print(\"‚úì Descompresi√≥n completada\")\n            return True\n        except subprocess.CalledProcessError as e:\n            print(f\"‚úó Error: {e}\")\n            return False\n    \n    def launch_parameter_server(self):\n        \"\"\"Lanza el Parameter Server\"\"\"\n        print(\"üöÄ Lanzando Parameter Server...\")\n        self.ps_log = open('/kaggle/working/ps_output.log', 'w')\n        \n        ps_process = subprocess.Popen(\n            # --- 2. USAMOS SYS.EXECUTABLE ---\n            [sys.executable, 'parameter_server.py', '--rank', '0', '--world_size', str(self.world_size)],\n            stdout=self.ps_log, \n            stderr=subprocess.STDOUT,\n            text=True\n        )\n        self.processes.append(ps_process)\n        return ps_process\n        \n    def launch_worker(self, rank):\n        \"\"\"Lanza un Worker\"\"\"\n        print(f\"üöÄ Lanzando Worker {rank}...\")\n        \n        # --- 3. USAMOS SYS.EXECUTABLE ---\n        worker_process = subprocess.Popen(\n            [sys.executable, 'worker.py', '--rank', str(rank), '--world_size', str(self.world_size)],\n            stdout=subprocess.PIPE, \n            stderr=subprocess.STDOUT,\n            text=True,\n            bufsize=1\n        )\n        self.processes.append(worker_process)\n        return worker_process\n        \n    def cleanup(self):\n        print(\"üßπ Limpiando procesos...\")\n        if hasattr(self, 'ps_log'):\n            self.ps_log.close()\n            \n        for p in self.processes:\n            if p.poll() is None:\n                p.terminate()\n                p.wait(timeout=5)\n\n    def run(self):\n        try:\n            if not self.prepare_data():\n                return False\n            \n            ps_process = self.launch_parameter_server()\n            print(\"‚è≥ Esperando 10s a que PS inicialice...\")\n            time.sleep(10)\n            \n            worker_processes = []\n            for rank in range(1, self.world_size):\n                worker_processes.append(self.launch_worker(rank))\n            \n            print(\"\\n\" + \"=\"*60)\n            print(\"üèãÔ∏è  ENTRENAMIENTO EN PROGRESO\")\n            print(\"=\"*60 + \"\\n\")\n            \n            main_worker = worker_processes[0]\n            for line in main_worker.stdout:\n                print(line, end='')\n            \n            for i, worker in enumerate(worker_processes, 1):\n                return_code = worker.wait()\n                print(f\"‚úì Worker {i} finaliz√≥ (c√≥digo: {return_code})\")\n            \n            print(\"\\n\" + \"=\"*60)\n            print(\"‚úÖ ENTRENAMIENTO COMPLETADO\")\n            print(\"=\"*60)\n            return True\n            \n        except KeyboardInterrupt:\n            print(\"\\n‚ö†Ô∏è  Interrupci√≥n detectada\")\n            return False\n        except Exception as e:\n            print(f\"\\n‚ùå Error: {e}\")\n            import traceback\n            traceback.print_exc()\n            return False\n        finally:\n            self.cleanup()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:36:59.851591Z","iopub.execute_input":"2025-10-24T16:36:59.851802Z","iopub.status.idle":"2025-10-24T16:36:59.863112Z","shell.execute_reply.started":"2025-10-24T16:36:59.851787Z","shell.execute_reply":"2025-10-24T16:36:59.862407Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"%%writefile launcher.py\nimport subprocess\nimport os\nimport time\nfrom pathlib import Path\nimport sys  # <--- 1. IMPORTAMOS SYS\n\nclass DistributedTrainingLauncher:\n    def __init__(self, world_size=2):\n        self.world_size = world_size\n        self.processes = []\n        \n    def prepare_data(self):\n        target_dir = Path(\"/kaggle/working/imagewoof2\")\n        train_dir = target_dir / \"train\"\n        tar_file = Path(\"/kaggle/input/imagewoof/imagewoof2.tgz\")\n        \n        if train_dir.exists():\n            print(f\"‚úì Dataset ya existe\")\n            return True\n        \n        print(f\"Descomprimiendo dataset...\")\n        target_dir.mkdir(parents=True, exist_ok=True)\n        \n        try:\n            subprocess.run(['tar', '-xzf', str(tar_file), '-C', '/kaggle/working/'], check=True)\n            print(\"‚úì Descompresi√≥n completada\")\n            return True\n        except subprocess.CalledProcessError as e:\n            print(f\"‚úó Error: {e}\")\n            return False\n    \n    def launch_parameter_server(self):\n        \"\"\"Lanza el Parameter Server\"\"\"\n        print(\"üöÄ Lanzando Parameter Server...\")\n        self.ps_log = open('/kaggle/working/ps_output.log', 'w')\n        \n        ps_process = subprocess.Popen(\n            # --- 2. USAMOS SYS.EXECUTABLE ---\n            [sys.executable, 'parameter_server.py', '--rank', '0', '--world_size', str(self.world_size)],\n            stdout=self.ps_log, \n            stderr=subprocess.STDOUT,\n            text=True\n        )\n        self.processes.append(ps_process)\n        return ps_process\n        \n    def launch_worker(self, rank):\n        \"\"\"Lanza un Worker\"\"\"\n        print(f\"üöÄ Lanzando Worker {rank}...\")\n        \n        # --- 3. USAMOS SYS.EXECUTABLE ---\n        worker_process = subprocess.Popen(\n            [sys.executable, 'worker.py', '--rank', str(rank), '--world_size', str(self.world_size)],\n            stdout=subprocess.PIPE, \n            stderr=subprocess.STDOUT,\n            text=True,\n            bufsize=1\n        )\n        self.processes.append(worker_process)\n        return worker_process\n        \n    def cleanup(self):\n        print(\"üßπ Limpiando procesos...\")\n        if hasattr(self, 'ps_log'):\n            self.ps_log.close()\n            \n        for p in self.processes:\n            if p.poll() is None:\n                p.terminate()\n                p.wait(timeout=5)\n\n    def run(self):\n        try:\n            if not self.prepare_data():\n                return False\n            \n            ps_process = self.launch_parameter_server()\n            print(\"‚è≥ Esperando 10s a que PS inicialice...\")\n            time.sleep(10)\n            \n            worker_processes = []\n            for rank in range(1, self.world_size):\n                worker_processes.append(self.launch_worker(rank))\n            \n            print(\"\\n\" + \"=\"*60)\n            print(\"üèãÔ∏è  ENTRENAMIENTO EN PROGRESO\")\n            print(\"=\"*60 + \"\\n\")\n            \n            main_worker = worker_processes[0]\n            for line in main_worker.stdout:\n                print(line, end='')\n            \n            for i, worker in enumerate(worker_processes, 1):\n                return_code = worker.wait()\n                print(f\"‚úì Worker {i} finaliz√≥ (c√≥digo: {return_code})\")\n            \n            print(\"\\n\" + \"=\"*60)\n            print(\"‚úÖ ENTRENAMIENTO COMPLETADO\")\n            print(\"=\"*60)\n            return True\n            \n        except KeyboardInterrupt:\n            print(\"\\n‚ö†Ô∏è  Interrupci√≥n detectada\")\n            return False\n        except Exception as e:\n            print(f\"\\n‚ùå Error: {e}\")\n            import traceback\n            traceback.print_exc()\n            return False\n        finally:\n            self.cleanup()","metadata":{}},{"cell_type":"code","source":"launcher = DistributedTrainingLauncher(world_size=2)\nsuccess = launcher.run()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:36:59.863793Z","iopub.execute_input":"2025-10-24T16:36:59.864034Z","iopub.status.idle":"2025-10-24T16:38:06.529202Z","shell.execute_reply.started":"2025-10-24T16:36:59.864019Z","shell.execute_reply":"2025-10-24T16:38:06.528365Z"}},"outputs":[{"name":"stdout","text":"‚úì Dataset ya existe\nüöÄ Lanzando Parameter Server...\n‚è≥ Esperando 10s a que PS inicialice...\nüöÄ Lanzando Worker 1...\n\n============================================================\nüèãÔ∏è  ENTRENAMIENTO EN PROGRESO\n============================================================\n\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n\n‚ö†Ô∏è  Interrupci√≥n detectada\nüßπ Limpiando procesos...\n","output_type":"stream"}],"execution_count":22}]}